## 构建一个大型语言模型 笔记

### 理解大语言模型

* 前言

  * 大语言模型（```LLM```），如```ChatGPT```是深度神经网络模型，基于深度学习（是机器学习和人工智能的一个子集）理论在海量文本数据上进行训练和支撑于```Transformer```架构，能够复杂理解进行上下文分析并生成符合上下文的连续文本。而早期自然语言处理（```NLP```），只能执行特定任务，比如在电子邮件上，```NLP```可以对其进行垃圾分类，但不是根据关键词列表撰写电子邮件。

* ```LLM```是什么

  * 大语言模型的“大”指的是模型参数规模大和训练数据量大。这类模型通常包含数十亿甚至百亿的参数，这些参数可调整权重，训练过程中通过优化来预测序列的下一个单词。
  * 支持的```Transformer```架构，能够在预测时对输入的不同部分进行选择性关注，因此非常适合处理人类语言的细微差别和复杂性。
  * 大语言模型与深度学习、机器模学习和人工智能的一些关系：

    ![](https://raw.githubusercontent.com/Garden12138/picbed-cloud/main/ai/llm_rels.png)

    如不同领域关系图所示，```LLMs```是深度学习技术的一个特定应用，发挥它们处理和生成人类语言文本的能力；深度学习是机器学习的一个专业分支，专注于多层神经网络来建模数据中的复杂模型和抽象。机器学习往往不需要明确的编程实现，而是涉及从数据中学习并基于数据作出预测或决策的算法研究，与深度学习不同的是传统机器学习需要手动提取特征。而人工智能领域除了由机器学习和深度学习主动，还涵盖了其他方法，如基于规则的系统、遗传算法、专家系统、模糊逻辑和符合推理。

* ```LLM```的应用
  
  * 广泛应用于机器翻译、新文本生成、情感分析、内容创作等。支持复杂的聊天机器人和虚拟助手，可以有效从医学、法律、金融等专业领域的大量文本中检索知识。

* 构建和使用```LLM```的步骤

  * 构建一个```LLM```的重要性：

    * 帮助理解其工作机制和局限性。
    * 使具备为特定任务领域定制```LLM```的能力。
    * 在数据隐私上实现保密。

  * 构建```LLM```的过程主要包含预训练和微调。预训练指在大型且多样化的数据集上进行训练，使模型具备语言理解的基础能力，包括语法、词汇和语言结构等，可以准备的预测下一个token。而微调则是在特定任务或领域的数据集上进行训练，使模型具备特定任务或领域的理解和生成能力。微调在范围上可分为全权重微调和冻结部份权重微调，前者是会对所有预训练权重进行调整，但调整幅度较小，保证原有的语言理解和生成能力，后者则冻结底层（往往是语言特征）权重，对高层权重进行调整；在类别上可分为指令微调和分类任务微调，前者为标注数据集包含指令和答案对，如翻译文本的查询机器正确方案，后者为标注数据集由文本及其对应类别标签组成，如入垃圾邮件和非垃圾邮件相关的电子邮件。

    ![](https://raw.githubusercontent.com/Garden12138/picbed-cloud/main/ai/llm_build.png)

* 介绍```Transformer```架构

  * 原始```Transformer```架构的简易示意图，以语言翻译为例：

    ![](https://raw.githubusercontent.com/Garden12138/picbed-cloud/main/ai/transformer.png)

    由两部分组成，一个是编码器，用于处理输入文本并生成嵌入表示（向量值），另一个是解码器，利用该表示逐字生成翻译文本。

  * ```Transformer```架构的后续变体```BERT```（双向编码器表示来自```Transformers```的缩写）和```GPT```（生成预训练```Transformers```的缩写）如下：

    ![](https://raw.githubusercontent.com/Garden12138/picbed-cloud/main/ai/bert_gpt.png)

    左侧的编码器部分类似```BERT```的```LLM```，右侧的解码器部分类似```GPT```的```LLM```。

    ```BERT```采用```masked language model（MLM）```训练方式，在训练过程中随机遮盖输入文本中的一部分，并预测被遮盖的部分，这种训练方式使```BERT```能更好理解的上下文，因为它需要根据前后部分来预测被遮掩的词，类似我们在英语考试中的完型填空，常应用于文档分类或情感预测等场景。而```GPT```更关注生成任务。

* 利用大型数据集

  * 这是```GPT-3 LLM```预训练数据集的规模：

    ![](https://raw.githubusercontent.com/Garden12138/picbed-cloud/main/ai/gpt3_llm_data.png)

    这个训练数据集的规模和多样性使得这些模型在不同语法、语义和上下文信息中表现优异，经过这个规模的数据集训练后，我们称这些模型为基础模型。由此可见，预训练```LLM```需要消耗大量资源和成本。所以在实际中我们一般都是使用开源的```LLM```模型进行微调，但我们仍然可以通过少量数据进行预训练，学习下预训练过程。

* 深入解析```GPT```架构

  * ```GPT```类型的解码器模型是一种自回归模型，自回归模型会将之前的输出作为未来预测的输出，因此所有下一个词的预测都是基于之前的文本，提高生成文本的连贯性。

    ![](https://raw.githubusercontent.com/Garden12138/picbed-cloud/main/ai/gpt_iteration.png)

* 构建大语言模型

  * 构建```LLM```的阶段包括数据准备和实现```LLM```框架，预训练```LLM```来创建基础模型，以及对基础模型进行微调，以适应特定任务或领域，如个人助手或文本分类器。

    ![](https://raw.githubusercontent.com/Garden12138/picbed-cloud/main/ai/build_llm_stage.png)